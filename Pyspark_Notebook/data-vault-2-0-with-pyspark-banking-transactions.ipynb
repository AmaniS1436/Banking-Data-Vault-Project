{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2743905,"sourceType":"datasetVersion","datasetId":1672910}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Data Vault 2.0 with PySpark – Banking Transactions Pipeline","metadata":{}},{"cell_type":"markdown","source":"# 1. Introduction\n","metadata":{}},{"cell_type":"markdown","source":"\nIn modern analytics, businesses need scalable and flexible data architectures.  \n**Data Vault 2.0** is a methodology for building **enterprise data warehouses** that are:  \n- **Scalable** → Handles large volumes of data.  \n- **Auditable** → Keeps history of changes.  \n- **Flexible** → Easy to adapt to new requirements.  \n\nUnlike traditional **Star Schema** (Facts & Dimensions only), Data Vault separates:  \n1. **Raw Vault** → Stores raw, historical data in 3 types of tables:\n   - **Hubs** → Unique business keys (e.g., Customer, Transaction, Location).  \n   - **Links** → Relationships between hubs (e.g., Customer–Transaction).  \n   - **Satellites** → Descriptive attributes with history (e.g., Customer profile, Transaction details).  \n\n2. **Business Vault** → Adds **derived attributes** and **business logic** (e.g., Age calculation, Fraud flags).  \n\n3. **Dimensional Model** → Final tables for **reporting and dashboards** in Power BI.  \n\nWe will:  \n- Build a **Data Vault 2.0** model in PySpark.  \n- Use **hash keys** (SHA-256) to generate unique IDs.  \n- Detect changes using **hash diff**.  \n- Export **dimensions and facts** for Power BI dashboards.","metadata":{}},{"cell_type":"markdown","source":"# 2. Environment Setup","metadata":{}},{"cell_type":"markdown","source":"We use **PySpark** as our processing engine.  \n\n- **SparkSession** → The entry point for using Spark.  \n- **Parquet** → Storage format for tables (efficient columnar storage).  \n- **Modes**:  \n  - `\"overwrite\"` → Deletes old data and replaces it.  \n  - `\"append\"` → Keeps history and adds new records.  ","metadata":{}},{"cell_type":"code","source":"import os\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import (\n    sha2, concat_ws, col, to_date, lpad, when, length,\n    split, lit, floor, months_between, current_date, year, month\n)\n\n# Initialize Spark session\nspark = SparkSession.builder \\\n    .appName(\"DataVaultBanking\") \\\n    .master(\"local[*]\") \\\n    .getOrCreate()\n\n# Define output paths\nOUTPUT_BASE = \"/content/datalake/raw_vault\"\nBV_BASE = \"/content/datalake/business_vault\"\n\nos.makedirs(OUTPUT_BASE, exist_ok=True)\nos.makedirs(BV_BASE, exist_ok=True)\n\nspark.sparkContext.setLogLevel(\"ERROR\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:31:02.250225Z","iopub.execute_input":"2025-09-24T21:31:02.250543Z","iopub.status.idle":"2025-09-24T21:31:02.260553Z","shell.execute_reply.started":"2025-09-24T21:31:02.250521Z","shell.execute_reply":"2025-09-24T21:31:02.259761Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# 3. Load Dataset","metadata":{}},{"cell_type":"markdown","source":"\nWe use a **banking dataset** containing:\n\n- **Customers** → ID, Name, Date of Birth, Balance.  \n- **Transactions** → ID, Date, Amount.  \n- **Locations** → Branch or city where the transaction took place.  \n\nThis dataset is the **source system**, and we will transform it into a Data Vault.  ","metadata":{}},{"cell_type":"code","source":"# Load dataset\ndf = spark.read.csv(\"/kaggle/input/bank-customer-segmentation/bank_transactions.csv\", header=True, inferSchema=True)\n\n# Preview first records\ndf.show(5, truncate=False)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:31:06.603686Z","iopub.execute_input":"2025-09-24T21:31:06.604517Z","iopub.status.idle":"2025-09-24T21:31:09.485860Z","shell.execute_reply.started":"2025-09-24T21:31:06.604484Z","shell.execute_reply":"2025-09-24T21:31:09.484098Z"}},"outputs":[{"name":"stderr","text":"[Stage 13:=============================>                            (2 + 2) / 4]\r","output_type":"stream"},{"name":"stdout","text":"+-------------+----------+-----------+----------+------------+------------------+---------------+---------------+-----------------------+\n|TransactionID|CustomerID|CustomerDOB|CustGender|CustLocation|CustAccountBalance|TransactionDate|TransactionTime|TransactionAmount (INR)|\n+-------------+----------+-----------+----------+------------+------------------+---------------+---------------+-----------------------+\n|T1           |C5841053  |10/1/94    |F         |JAMSHEDPUR  |17819.05          |2/8/16         |143207         |25.0                   |\n|T2           |C2142763  |4/4/57     |M         |JHAJJAR     |2270.69           |2/8/16         |141858         |27999.0                |\n|T3           |C4417068  |26/11/96   |F         |MUMBAI      |17874.44          |2/8/16         |142712         |459.0                  |\n|T4           |C5342380  |14/9/73    |F         |MUMBAI      |866503.21         |2/8/16         |142714         |2060.0                 |\n|T5           |C9031234  |24/3/88    |F         |NAVI MUMBAI |6714.43           |2/8/16         |181156         |1762.5                 |\n+-------------+----------+-----------+----------+------------+------------------+---------------+---------------+-----------------------+\nonly showing top 5 rows\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# 4. Raw Vault Layer Construction","metadata":{}},{"cell_type":"markdown","source":"The Raw Vault is the foundation of the Data Vault 2.0 methodology.  \nIt ensures traceability, flexibility, and historization of the data.  \n\nIt is built on three key components:  \n\n### Hubs – Unique Business Keys  \n- Represent the core business entities.  \n- Each hub stores a unique list of business keys.  \n- Examples in our case:  \n  - Customer Hub → list of unique customers.  \n  - Transaction Hub → list of unique transactions.  \n  - (Optional) Location Hub → list of unique locations.  \n- Technical implementation:  \n  - Use SHA-256 hashing to generate the hub’s primary key (hash key).  \n  - Why SHA-256? → Strong cryptographic hash ensures uniqueness and minimizes the risk of collisions.  \n\n---\n\n### Links – Relationships Between Hubs  \n- Represent associations between business entities.  \n- Example:  \n  - A link between Customer Hub and Transaction Hub → indicates which customer performed which transaction.  \n- Technical implementation:  \n  - The link’s hash key is created using a concatenation of hub keys followed by SHA-256 hashing.  \n\n---\n\n### Satellites – Descriptive Attributes  \n- Store contextual and descriptive information about a hub or link.  \n- Examples in our case:  \n  - Customer Satellite → Name, Date of Birth, Gender, Account Balance.  \n  - Transaction Satellite → Amount, Date, Type of transaction.  \n- Technical implementation:  \n  - Use a hash diff to detect attribute changes over time.  \n  - Store additional metadata:  \n    - load_date → when the record was inserted.  \n    - record_source → origin of the record (CSV, API, system).  \n- This guarantees historization and traceability of changes.  \n\n---\n\n**Summary**:  \n- Hubs = Who/What (unique identifiers).  \n- Links = How they are related.  \n- Satellites = Descriptive details and history.  \n","metadata":{}},{"cell_type":"markdown","source":"**Step 1 : Path variables**","metadata":{}},{"cell_type":"code","source":"# Input dataset \nINPUT_CSV = \"/kaggle/input/bank-customer-segmentation\"\n\n# Staging Layer path\nSTAGING_PATH = \"/kaggle/working/datalake/staging/bank_transactions\"\n\n# Raw Vault Layer path\nOUTPUT_BASE = \"/kaggle/working/datalake/raw_vault\"\n\n# Create directories if they do not exist\nos.makedirs(STAGING_PATH, exist_ok=True)\nos.makedirs(OUTPUT_BASE, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:31:18.466159Z","iopub.execute_input":"2025-09-24T21:31:18.466458Z","iopub.status.idle":"2025-09-24T21:31:18.472010Z","shell.execute_reply.started":"2025-09-24T21:31:18.466435Z","shell.execute_reply":"2025-09-24T21:31:18.471189Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"**Step 2 : Loading Raw Data**","metadata":{}},{"cell_type":"code","source":"df = spark.read.csv(INPUT_CSV, header=True, inferSchema=True)\n\nprint(\"Initial schema:\")\ndf.printSchema()\ndf.show(18, truncate=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:31:21.663339Z","iopub.execute_input":"2025-09-24T21:31:21.663605Z","iopub.status.idle":"2025-09-24T21:31:23.771999Z","shell.execute_reply.started":"2025-09-24T21:31:21.663587Z","shell.execute_reply":"2025-09-24T21:31:23.771129Z"}},"outputs":[{"name":"stderr","text":"[Stage 16:==============>                                           (1 + 3) / 4]\r","output_type":"stream"},{"name":"stdout","text":"Initial schema:\nroot\n |-- TransactionID: string (nullable = true)\n |-- CustomerID: string (nullable = true)\n |-- CustomerDOB: string (nullable = true)\n |-- CustGender: string (nullable = true)\n |-- CustLocation: string (nullable = true)\n |-- CustAccountBalance: double (nullable = true)\n |-- TransactionDate: string (nullable = true)\n |-- TransactionTime: integer (nullable = true)\n |-- TransactionAmount (INR): double (nullable = true)\n\n+-------------+----------+-----------+----------+------------+------------------+---------------+---------------+-----------------------+\n|TransactionID|CustomerID|CustomerDOB|CustGender|CustLocation|CustAccountBalance|TransactionDate|TransactionTime|TransactionAmount (INR)|\n+-------------+----------+-----------+----------+------------+------------------+---------------+---------------+-----------------------+\n|T1           |C5841053  |10/1/94    |F         |JAMSHEDPUR  |17819.05          |2/8/16         |143207         |25.0                   |\n|T2           |C2142763  |4/4/57     |M         |JHAJJAR     |2270.69           |2/8/16         |141858         |27999.0                |\n|T3           |C4417068  |26/11/96   |F         |MUMBAI      |17874.44          |2/8/16         |142712         |459.0                  |\n|T4           |C5342380  |14/9/73    |F         |MUMBAI      |866503.21         |2/8/16         |142714         |2060.0                 |\n|T5           |C9031234  |24/3/88    |F         |NAVI MUMBAI |6714.43           |2/8/16         |181156         |1762.5                 |\n|T6           |C1536588  |8/10/72    |F         |ITANAGAR    |53609.2           |2/8/16         |173940         |676.0                  |\n|T7           |C7126560  |26/1/92    |F         |MUMBAI      |973.46            |2/8/16         |173806         |566.0                  |\n|T8           |C1220223  |27/1/82    |M         |MUMBAI      |95075.54          |2/8/16         |170537         |148.0                  |\n|T9           |C8536061  |19/4/88    |F         |GURGAON     |14906.96          |2/8/16         |192825         |833.0                  |\n|T10          |C6638934  |22/6/84    |M         |MUMBAI      |4279.22           |2/8/16         |192446         |289.11                 |\n|T11          |C5430833  |22/7/82    |M         |MOHALI      |48429.49          |2/8/16         |204133         |259.0                  |\n|T12          |C6939838  |7/7/88     |M         |GUNTUR      |14613.46          |2/8/16         |205108         |202.0                  |\n|T13          |C6339347  |13/6/78    |M         |AHMEDABAD   |32274.78          |2/8/16         |203834         |12300.0                |\n|T14          |C8327851  |5/1/92     |F         |THANE       |59950.44          |1/8/16         |84706          |50.0                   |\n|T15          |C7917151  |24/3/78    |M         |PUNE        |10100.84          |1/8/16         |82253          |338.0                  |\n|T16          |C8334633  |10/7/68    |F         |NEW DELHI   |1283.12           |1/8/16         |125725         |250.0                  |\n|T17          |C1376215  |1/1/1800   |M         |MUMBAI      |77495.15          |1/8/16         |124727         |1423.11                |\n|T18          |C8967349  |16/7/89    |M         |MUMBAI      |2177.85           |1/8/16         |124734         |54.0                   |\n+-------------+----------+-----------+----------+------------+------------------+---------------+---------------+-----------------------+\nonly showing top 18 rows\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"**Step 3 : Data Cleaning and Normalization**\n","metadata":{}},{"cell_type":"markdown","source":"Before creating the Data Vault structures, we need clean and consistent data.  \nThe following transformations are applied:\n\n1. **Standardizing dates**:  \n   - `CustomerDOB` and `TransactionDate` are reformatted into `YYYY-MM-DD`.  \n   - Two-digit years are corrected (e.g., \"85\" → \"1985\").  \n\n2. **Filtering invalid dates**:  \n   - Customer DOB must be between `1900-01-01` and `2025-12-31`.  \n   - Transaction dates must be between `2000-01-01` and `2025-12-31`.  \n\n3. **Casting numeric types**:  \n   - `CustAccountBalance` and `TransactionAmount` are cast to `double`.  \n   - `TransactionTime` is stored as `string`.  \n\n4. **Staging layer**:  \n   The cleaned dataset is saved into the **Staging Layer** (`/datalake/staging`) in Parquet format for traceability.  \n","metadata":{}},{"cell_type":"code","source":"# --- Standardize CustomerDOB ---\ndf = df.withColumn(\"dob_parts\", split(col(\"CustomerDOB\"), \"/\")) \\\n       .withColumn(\"dob_day\", lpad(col(\"dob_parts\")[0], 2, \"0\")) \\\n       .withColumn(\"dob_month\", lpad(col(\"dob_parts\")[1], 2, \"0\")) \\\n       .withColumn(\n           \"dob_year\",\n           when(length(col(\"dob_parts\")[2]) == 2, concat_ws(\"\", lit(\"19\"), col(\"dob_parts\")[2]))\n           .otherwise(col(\"dob_parts\")[2])\n       ) \\\n       .withColumn(\"CustomerDOB\", to_date(concat_ws(\"/\", col(\"dob_day\"), col(\"dob_month\"), col(\"dob_year\")), \"dd/MM/yyyy\")) \\\n       .drop(\"dob_parts\", \"dob_day\", \"dob_month\", \"dob_year\")\n\n# --- Standardize TransactionDate ---\ndf = df.withColumn(\"tx_parts\", split(col(\"TransactionDate\"), \"/\")) \\\n       .withColumn(\"tx_day\", lpad(col(\"tx_parts\")[0], 2, \"0\")) \\\n       .withColumn(\"tx_month\", lpad(col(\"tx_parts\")[1], 2, \"0\")) \\\n       .withColumn(\n           \"tx_year\",\n           when(length(col(\"tx_parts\")[2]) == 2, concat_ws(\"\", lit(\"20\"), col(\"tx_parts\")[2]))\n           .otherwise(col(\"tx_parts\")[2])\n       ) \\\n       .withColumn(\"TransactionDate\", to_date(concat_ws(\"/\", col(\"tx_day\"), col(\"tx_month\"), col(\"tx_year\")), \"dd/MM/yyyy\")) \\\n       .drop(\"tx_parts\", \"tx_day\", \"tx_month\", \"tx_year\")\n\n# --- Filter invalid dates ---\ndf = df.filter((col(\"CustomerDOB\") >= \"1900-01-01\") & (col(\"CustomerDOB\") <= \"2025-12-31\")) \\\n       .filter((col(\"TransactionDate\") >= \"2000-01-01\") & (col(\"TransactionDate\") <= \"2025-12-31\"))\n\n# --- Convert numeric types ---\ndf = df.withColumn(\"CustAccountBalance\", col(\"CustAccountBalance\").cast(\"double\")) \\\n       .withColumn(\"TransactionTime\", col(\"TransactionTime\").cast(\"string\")) \\\n       .withColumn(\"TransactionAmount (INR)\", col(\"TransactionAmount (INR)\").cast(\"double\"))\n\nprint(\"Schema after cleaning:\")\ndf.printSchema()\ndf.show(18, truncate=False)\n\n# Save cleaned data into Staging Layer\ndf.write.mode(\"overwrite\").parquet(STAGING_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:31:45.087703Z","iopub.execute_input":"2025-09-24T21:31:45.088280Z","iopub.status.idle":"2025-09-24T21:31:56.499191Z","shell.execute_reply.started":"2025-09-24T21:31:45.088254Z","shell.execute_reply":"2025-09-24T21:31:56.498164Z"}},"outputs":[{"name":"stdout","text":"Schema after cleaning:\nroot\n |-- TransactionID: string (nullable = true)\n |-- CustomerID: string (nullable = true)\n |-- CustomerDOB: date (nullable = true)\n |-- CustGender: string (nullable = true)\n |-- CustLocation: string (nullable = true)\n |-- CustAccountBalance: double (nullable = true)\n |-- TransactionDate: date (nullable = true)\n |-- TransactionTime: string (nullable = true)\n |-- TransactionAmount (INR): double (nullable = true)\n\n+-------------+----------+-----------+----------+------------+------------------+---------------+---------------+-----------------------+\n|TransactionID|CustomerID|CustomerDOB|CustGender|CustLocation|CustAccountBalance|TransactionDate|TransactionTime|TransactionAmount (INR)|\n+-------------+----------+-----------+----------+------------+------------------+---------------+---------------+-----------------------+\n|T1           |C5841053  |1994-01-10 |F         |JAMSHEDPUR  |17819.05          |2016-08-02     |143207         |25.0                   |\n|T2           |C2142763  |1957-04-04 |M         |JHAJJAR     |2270.69           |2016-08-02     |141858         |27999.0                |\n|T3           |C4417068  |1996-11-26 |F         |MUMBAI      |17874.44          |2016-08-02     |142712         |459.0                  |\n|T4           |C5342380  |1973-09-14 |F         |MUMBAI      |866503.21         |2016-08-02     |142714         |2060.0                 |\n|T5           |C9031234  |1988-03-24 |F         |NAVI MUMBAI |6714.43           |2016-08-02     |181156         |1762.5                 |\n|T6           |C1536588  |1972-10-08 |F         |ITANAGAR    |53609.2           |2016-08-02     |173940         |676.0                  |\n|T7           |C7126560  |1992-01-26 |F         |MUMBAI      |973.46            |2016-08-02     |173806         |566.0                  |\n|T8           |C1220223  |1982-01-27 |M         |MUMBAI      |95075.54          |2016-08-02     |170537         |148.0                  |\n|T9           |C8536061  |1988-04-19 |F         |GURGAON     |14906.96          |2016-08-02     |192825         |833.0                  |\n|T10          |C6638934  |1984-06-22 |M         |MUMBAI      |4279.22           |2016-08-02     |192446         |289.11                 |\n|T11          |C5430833  |1982-07-22 |M         |MOHALI      |48429.49          |2016-08-02     |204133         |259.0                  |\n|T12          |C6939838  |1988-07-07 |M         |GUNTUR      |14613.46          |2016-08-02     |205108         |202.0                  |\n|T13          |C6339347  |1978-06-13 |M         |AHMEDABAD   |32274.78          |2016-08-02     |203834         |12300.0                |\n|T14          |C8327851  |1992-01-05 |F         |THANE       |59950.44          |2016-08-01     |84706          |50.0                   |\n|T15          |C7917151  |1978-03-24 |M         |PUNE        |10100.84          |2016-08-01     |82253          |338.0                  |\n|T16          |C8334633  |1968-07-10 |F         |NEW DELHI   |1283.12           |2016-08-01     |125725         |250.0                  |\n|T18          |C8967349  |1989-07-16 |M         |MUMBAI      |2177.85           |2016-08-01     |124734         |54.0                   |\n|T19          |C3732016  |1991-01-11 |M         |MUMBAI      |32816.17          |2016-08-01     |122135         |315.0                  |\n+-------------+----------+-----------+----------+------------+------------------+---------------+---------------+-----------------------+\nonly showing top 18 rows\n\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"**Step 4 : Data Vault - Hubs**","metadata":{}},{"cell_type":"markdown","source":"Hubs contain the **unique list of business keys**.  \n- **Customer Hub** → based on `CustomerID`.  \n- **Transaction Hub** → based on `TransactionID`.  \n\nWe use **SHA-256 hashing** to generate surrogate keys (`hk_customer_id`, `hk_transaction_id`).  \nThis guarantees uniqueness and avoids collisions.  ","metadata":{}},{"cell_type":"code","source":"# Hub Customer\nhub_customer = df.select(\"CustomerID\").dropDuplicates() \\\n                 .withColumn(\"hk_customer_id\", sha2(col(\"CustomerID\").cast(\"string\"), 256))\nhub_customer.write.mode(\"append\").parquet(f\"{OUTPUT_BASE}/hub_customer\")\n\n# Hub Transaction\nhub_transaction = df.select(\"TransactionID\").dropDuplicates() \\\n                    .withColumn(\"hk_transaction_id\", sha2(col(\"TransactionID\").cast(\"string\"), 256))\nhub_transaction.write.mode(\"append\").parquet(f\"{OUTPUT_BASE}/hub_transaction\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:34:41.244499Z","iopub.execute_input":"2025-09-24T21:34:41.244757Z","iopub.status.idle":"2025-09-24T21:35:00.643395Z","shell.execute_reply.started":"2025-09-24T21:34:41.244740Z","shell.execute_reply":"2025-09-24T21:35:00.641377Z"}},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"**Step 5 : Data Vault - Links**","metadata":{}},{"cell_type":"markdown","source":"Links capture the **relationships between business keys**.  \nIn our case:  \n- **Customer ↔ Transaction** link → which customer performed which transaction.  \n\nA hash key (`hk_link_cust_txn`) is generated by combining `CustomerID` and `TransactionID` with SHA-256.  ","metadata":{}},{"cell_type":"code","source":"link_cust_txn = df.select(\"CustomerID\", \"TransactionID\").dropDuplicates() \\\n                  .withColumn(\n                      \"hk_link_cust_txn\",\n                      sha2(concat_ws(\"||\",\n                                     col(\"CustomerID\").cast(\"string\"),\n                                     col(\"TransactionID\").cast(\"string\")), 256)\n                  )\nlink_cust_txn.write.mode(\"append\").parquet(f\"{OUTPUT_BASE}/link_cust_transaction\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:37:29.006678Z","iopub.execute_input":"2025-09-24T21:37:29.006992Z","iopub.status.idle":"2025-09-24T21:37:40.942459Z","shell.execute_reply.started":"2025-09-24T21:37:29.006964Z","shell.execute_reply":"2025-09-24T21:37:40.941594Z"}},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"**Step 6 : Data Vault - Satellites**","metadata":{}},{"cell_type":"markdown","source":"Satellites store the **descriptive attributes** related to hubs and links.  \nWe create two satellites:\n\n- **Customer Satellite** → attributes like DOB, gender, location, account balance.  \n- **Transaction Satellite** → attributes like transaction date, time, and amount.  \n\nEach satellite contains:  \n- **hash_diff** → to detect changes in descriptive attributes.  \n- **load_date** → timestamp of when the record was inserted.  \n- **record_source** → source system of the record (e.g., `\"bank_transactions_csv_v1\"`).  \n","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import *\nRECORD_SOURCE = \"bank_transactions_csv_v1\"\n\n# Satellite Customer\nsat_customer = df.select(\n    \"CustomerID\", \"CustomerDOB\", \"CustGender\", \"CustLocation\", \"CustAccountBalance\"\n).dropDuplicates([\"CustomerID\"]) \\\n .withColumn(\"hk_customer_id\", sha2(col(\"CustomerID\").cast(\"string\"), 256)) \\\n .withColumn(\"hash_diff\",\n             sha2(concat_ws(\"||\",\n                            col(\"CustomerDOB\").cast(\"string\"),\n                            col(\"CustGender\").cast(\"string\"),\n                            col(\"CustLocation\").cast(\"string\"),\n                            col(\"CustAccountBalance\").cast(\"string\")), 256)) \\\n .withColumn(\"load_date\", current_timestamp()) \\\n .withColumn(\"record_source\", lit(RECORD_SOURCE))\nsat_customer.write.mode(\"append\").parquet(f\"{OUTPUT_BASE}/sat_customer\")\n\n# Satellite Transaction\nsat_transaction = df.select(\n    \"TransactionID\", \"TransactionDate\", \"TransactionTime\", col(\"TransactionAmount (INR)\").alias(\"TransactionAmount\")\n).dropDuplicates([\"TransactionID\"]) \\\n .withColumn(\"hk_transaction_id\", sha2(col(\"TransactionID\").cast(\"string\"), 256)) \\\n .withColumn(\"hash_diff\",\n             sha2(concat_ws(\"||\",\n                            col(\"TransactionDate\").cast(\"string\"),\n                            col(\"TransactionTime\").cast(\"string\"),\n                            col(\"TransactionAmount\").cast(\"string\")), 256)) \\\n .withColumn(\"load_date\", current_timestamp()) \\\n .withColumn(\"record_source\", lit(RECORD_SOURCE))\nsat_transaction.write.mode(\"append\").parquet(f\"{OUTPUT_BASE}/sat_transaction\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:42:21.428775Z","iopub.execute_input":"2025-09-24T21:42:21.429436Z","iopub.status.idle":"2025-09-24T21:42:56.167334Z","shell.execute_reply.started":"2025-09-24T21:42:21.429410Z","shell.execute_reply":"2025-09-24T21:42:56.165849Z"}},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"**Step 7 : Final Verification**","metadata":{}},{"cell_type":"markdown","source":"We query the created Data Vault components to confirm successful loading:  \n- **Hub Customer**  \n- **Hub Transaction**  \n- **Link Customer ↔ Transaction**  \n- **Satellite Customer**  \n- **Satellite Transaction**  \n\nEach table is read back from the Raw Vault directory and a sample of rows is displayed.  \nThis step ensures the Data Vault structures were built correctly and can be used for downstream layers (Business Vault, Information Marts).  \n","metadata":{}},{"cell_type":"code","source":"print(\"===== HUB CUSTOMER =====\")\nspark.read.parquet(f\"{OUTPUT_BASE}/hub_customer\").show(5)\n\nprint(\"===== HUB TRANSACTION =====\")\nspark.read.parquet(f\"{OUTPUT_BASE}/hub_transaction\").show(5)\n\nprint(\"===== LINK CUSTOMER - TRANSACTION =====\")\nspark.read.parquet(f\"{OUTPUT_BASE}/link_cust_transaction\").show(5)\n\nprint(\"===== SAT CUSTOMER =====\")\nspark.read.parquet(f\"{OUTPUT_BASE}/sat_customer\").show(5)\n\nprint(\"===== SAT TRANSACTION =====\")\nspark.read.parquet(f\"{OUTPUT_BASE}/sat_transaction\").show(5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:44:10.156628Z","iopub.execute_input":"2025-09-24T21:44:10.157338Z","iopub.status.idle":"2025-09-24T21:44:11.584333Z","shell.execute_reply.started":"2025-09-24T21:44:10.157312Z","shell.execute_reply":"2025-09-24T21:44:11.583281Z"}},"outputs":[{"name":"stdout","text":"===== HUB CUSTOMER =====\n+----------+--------------------+\n|CustomerID|      hk_customer_id|\n+----------+--------------------+\n|  C5841053|0917d4201b7ffddc0...|\n|  C3232257|fee13cfb2fd583541...|\n|  C5310729|409cde3d1ee946fa8...|\n|  C7014439|c188f31176370f1b9...|\n|  C8393123|9bc9b87b3630be653...|\n+----------+--------------------+\nonly showing top 5 rows\n\n===== HUB TRANSACTION =====\n+-------------+--------------------+\n|TransactionID|   hk_transaction_id|\n+-------------+--------------------+\n|         T215|415c994cd9fb163e6...|\n|         T744|897d9ef3605395681...|\n|         T788|8fb51b4a9281c0df8...|\n|        T1599|1da1a25cc6cb59678...|\n|        T1668|34e83c1cf34e6befb...|\n+-------------+--------------------+\nonly showing top 5 rows\n\n===== LINK CUSTOMER - TRANSACTION =====\n+----------+-------------+--------------------+\n|CustomerID|TransactionID|    hk_link_cust_txn|\n+----------+-------------+--------------------+\n|  C8334633|          T16|5db0dad38783b72b8...|\n|  C5529374|          T78|3798cd5cab5af083f...|\n|  C6612422|          T80|f44d34038bd1034db...|\n|  C8439216|         T273|ce75ccdbe8c1bcd82...|\n|  C7325318|         T302|b0ff7f12bf962e41a...|\n+----------+-------------+--------------------+\nonly showing top 5 rows\n\n===== SAT CUSTOMER =====\n+----------+-----------+----------+------------+------------------+--------------------+--------------------+--------------------+--------------------+\n|CustomerID|CustomerDOB|CustGender|CustLocation|CustAccountBalance|      hk_customer_id|           hash_diff|           load_date|       record_source|\n+----------+-----------+----------+------------+------------------+--------------------+--------------------+--------------------+--------------------+\n|  C1010031| 1984-07-21|         M|        VAPI|            1754.1|7a82878dee0e04508...|3c0f7d5fe524a2b15...|2025-09-24 21:42:...|bank_transactions...|\n|  C1010038| 1992-07-13|         F|       LOHIT|           1290.76|98cecfe0593d2cdaf...|2e3e95ca169eeee7c...|2025-09-24 21:42:...|bank_transactions...|\n|  C1010045| 1987-05-21|         M|   AHMEDABAD|           7051.72|b3f5be13ba8a7fb38...|3fb5415d21cfa027e...|2025-09-24 21:42:...|bank_transactions...|\n|  C1010046| 1992-05-09|         F|     GURGAON|           2360.67|4204ac52e9466cab3...|bc43b8006f6d79b36...|2025-09-24 21:42:...|bank_transactions...|\n|  C1010050| 1988-07-12|         F|       DELHI|            122.08|a471a86ca741d5e77...|cbbcee65e6406ec07...|2025-09-24 21:42:...|bank_transactions...|\n+----------+-----------+----------+------------+------------------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 5 rows\n\n===== SAT TRANSACTION =====\n+-------------+---------------+---------------+-----------------+--------------------+--------------------+--------------------+--------------------+\n|TransactionID|TransactionDate|TransactionTime|TransactionAmount|   hk_transaction_id|           hash_diff|           load_date|       record_source|\n+-------------+---------------+---------------+-----------------+--------------------+--------------------+--------------------+--------------------+\n|         T100|     2016-08-06|         182603|            810.0|ea52f8f988a425775...|927fb022a0a4a5c08...|2025-09-24 21:42:...|bank_transactions...|\n|     T1000002|     2016-09-14|         114630|             30.0|2caf2b793259605db...|1d1942200be697a05...|2025-09-24 21:42:...|bank_transactions...|\n|     T1000004|     2016-09-14|         141538|            22.25|6fd948cbed0a5b64a...|25a81543c6b54ce6e...|2025-09-24 21:42:...|bank_transactions...|\n|     T1000007|     2016-09-14|         112352|           2000.0|b6aca904a073c3627...|8f8fe103ca33c7387...|2025-09-24 21:42:...|bank_transactions...|\n|     T1000009|     2016-09-14|         112032|            175.0|720c3476cc737f9bc...|95caae4f55b8309c5...|2025-09-24 21:42:...|bank_transactions...|\n+-------------+---------------+---------------+-----------------+--------------------+--------------------+--------------------+--------------------+\nonly showing top 5 rows\n\n","output_type":"stream"}],"execution_count":24},{"cell_type":"markdown","source":"# 5. Business Vault Layer Construction","metadata":{}},{"cell_type":"markdown","source":"The Business Vault is built on top of the Raw Vault.  \nIt enriches the data with **derived attributes, business rules, and metrics**.  \nThis makes it easier to prepare Information Marts for analytics and reporting.\n\nWe will create:\n1. **Customer Profile Enrichment** (age, balance categories, senior flag)  \n2. **Enriched Transactions** (categories, suspicious flag)  \n3. **Monthly Aggregates** (total transactions, amounts, customers)  \n4. **Customer-Level Aggregates** (number of transactions, total & avg amounts)  \n5. **Customer Segmentation** (Premium, Active, Occasional, etc.)\n","metadata":{}},{"cell_type":"markdown","source":"**Step 1 : Import libraries**","metadata":{}},{"cell_type":"code","source":"from pyspark.sql import functions as F\nfrom pyspark.sql.functions import col, year, month, current_date, months_between, floor, when, lit\nimport os","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:56:43.133173Z","iopub.execute_input":"2025-09-24T21:56:43.133483Z","iopub.status.idle":"2025-09-24T21:56:43.138189Z","shell.execute_reply.started":"2025-09-24T21:56:43.133464Z","shell.execute_reply":"2025-09-24T21:56:43.137209Z"}},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":"**Step 2 : Path Variables**","metadata":{}},{"cell_type":"code","source":"BV_BASE = \"/kaggle/working/datalake/business_vault\"\nos.makedirs(BV_BASE, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T21:57:53.734610Z","iopub.execute_input":"2025-09-24T21:57:53.735395Z","iopub.status.idle":"2025-09-24T21:57:53.739466Z","shell.execute_reply.started":"2025-09-24T21:57:53.735371Z","shell.execute_reply":"2025-09-24T21:57:53.738240Z"}},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":"**Step 3 : Customer Profile Enrichment**","metadata":{}},{"cell_type":"markdown","source":"We enrich the customer hub with additional attributes:\n- **Customer Age**: calculated from date of birth  \n- **Balance Category**: Low, Medium, High  \n- **IsSenior**: flag if age ≥ 65  \n\nThis creates a **Customer Profile table** ready for segmentation.  \n","metadata":{}},{"cell_type":"code","source":"# Remove hash key to avoid duplicates\nsat_customer_clean = sat_customer.drop(\"hk_customer_id\")\n\nbv_customer_profile = hub_customer.join(\n    sat_customer_clean,\n    on=\"CustomerID\",\n    how=\"left\"\n)\n\n# Derive customer age\nbv_customer_profile = bv_customer_profile.withColumn(\n    \"CustomerAge\",\n    floor(months_between(current_date(), col(\"CustomerDOB\")) / 12)\n)\n\n# Balance category\nbv_customer_profile = bv_customer_profile.withColumn(\n    \"BalanceCategory\",\n    when(col(\"CustAccountBalance\").isNull(), lit(\"Unknown\"))\n    .when(col(\"CustAccountBalance\") < 1000, lit(\"Low\"))\n    .when((col(\"CustAccountBalance\") >= 1000) & (col(\"CustAccountBalance\") < 50000), lit(\"Medium\"))\n    .otherwise(lit(\"High\"))\n)\n\n# Senior flag\nbv_customer_profile = bv_customer_profile.withColumn(\n    \"IsSenior\",\n    when(col(\"CustomerAge\") >= 65, lit(True)).otherwise(lit(False))\n)\n\n# Save enriched profile\nBV_CUSTOMER_PATH = f\"{BV_BASE}/bv_customer_profile\"\nbv_customer_profile.write.mode(\"overwrite\").parquet(BV_CUSTOMER_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T22:00:16.239508Z","iopub.execute_input":"2025-09-24T22:00:16.239783Z","iopub.status.idle":"2025-09-24T22:00:40.502107Z","shell.execute_reply.started":"2025-09-24T22:00:16.239765Z","shell.execute_reply":"2025-09-24T22:00:40.501142Z"}},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"**Step 4 : Enriched Transactions**","metadata":{}},{"cell_type":"markdown","source":"We add business logic to transactions:\n- **TransactionCategory**: Small, Medium, Large  \n- **IsSuspicious**: transactions above 100,000 flagged as suspicious  \n\nThis table helps with fraud detection and transaction monitoring.  \n","metadata":{}},{"cell_type":"code","source":"# Remove duplicate hash keys\nsat_transaction_clean = sat_transaction.drop(\"hk_transaction_id\")\n\ntx_with_customer = sat_transaction_clean.join(\n    link_cust_txn.select(\"TransactionID\", \"CustomerID\"),\n    on=\"TransactionID\",\n    how=\"left\"\n)\n\n# Transaction category\ntx_with_customer = tx_with_customer.withColumn(\n    \"TransactionCategory\",\n    when(col(\"TransactionAmount\").isNull(), lit(\"Unknown\"))\n    .when(col(\"TransactionAmount\") < 1000, lit(\"Small\"))\n    .when((col(\"TransactionAmount\") >= 1000) & (col(\"TransactionAmount\") < 10000), lit(\"Medium\"))\n    .otherwise(lit(\"Large\"))\n)\n\n# Suspicious flag\ntx_with_customer = tx_with_customer.withColumn(\n    \"IsSuspicious\",\n    when(col(\"TransactionAmount\") > 100000, lit(True)).otherwise(lit(False))\n)\n\n# Save\nBV_TX_PATH = f\"{BV_BASE}/bv_transaction_enriched\"\ntx_with_customer.write.mode(\"overwrite\").parquet(BV_TX_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T22:03:02.150379Z","iopub.execute_input":"2025-09-24T22:03:02.150658Z","iopub.status.idle":"2025-09-24T22:03:29.249929Z","shell.execute_reply.started":"2025-09-24T22:03:02.150640Z","shell.execute_reply":"2025-09-24T22:03:29.248780Z"}},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":32},{"cell_type":"markdown","source":"**Step 5 : Monthly Aggregates**","metadata":{}},{"cell_type":"markdown","source":"We group transactions by year and month to compute:\n- **TotalTransactions**  \n- **TotalAmount**  \n- **AverageAmount**  \n- **ActiveCustomers** (unique customers per month)  \n\nThis table supports time-series analysis of banking activity. ","metadata":{}},{"cell_type":"code","source":"tx_for_agg = tx_with_customer.withColumn(\"tx_year\", year(col(\"TransactionDate\"))) \\\n                             .withColumn(\"tx_month\", month(col(\"TransactionDate\")))\n\nbv_tx_monthly = tx_for_agg.groupBy(\"tx_year\", \"tx_month\").agg(\n    F.count(\"TransactionID\").alias(\"TotalTransactions\"),\n    F.sum(\"TransactionAmount\").alias(\"TotalAmount\"),\n    F.avg(\"TransactionAmount\").alias(\"AvgAmount\"),\n    F.countDistinct(\"CustomerID\").alias(\"ActiveCustomers\")\n).orderBy(\"tx_year\", \"tx_month\")\n\nBV_TX_MONTHLY_PATH = f\"{BV_BASE}/bv_transaction_monthly\"\nbv_tx_monthly.write.mode(\"overwrite\").parquet(BV_TX_MONTHLY_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T22:07:21.784998Z","iopub.execute_input":"2025-09-24T22:07:21.785380Z","iopub.status.idle":"2025-09-24T22:07:46.629537Z","shell.execute_reply.started":"2025-09-24T22:07:21.785357Z","shell.execute_reply":"2025-09-24T22:07:46.628676Z"}},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":33},{"cell_type":"markdown","source":"**Step 6 : Aggregates by Customer**","metadata":{}},{"cell_type":"markdown","source":"We compute customer-level aggregates:\n- **NbTransactions**  \n- **TotalTransactionAmount**  \n- **AvgTransactionAmount**  \n\nThis prepares the ground for segmentation and customer behavior analysis.  \n","metadata":{}},{"cell_type":"code","source":"bv_tx_by_customer = tx_for_agg.groupBy(\"CustomerID\").agg(\n    F.count(\"TransactionID\").alias(\"NbTransactions\"),\n    F.sum(\"TransactionAmount\").alias(\"TotalTransactionAmount\"),\n    F.avg(\"TransactionAmount\").alias(\"AvgTransactionAmount\")\n)\n\nBV_TX_BY_CUST_PATH = f\"{BV_BASE}/bv_transaction_by_customer\"\nbv_tx_by_customer.write.mode(\"overwrite\").parquet(BV_TX_BY_CUST_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T22:09:11.622644Z","iopub.execute_input":"2025-09-24T22:09:11.622908Z","iopub.status.idle":"2025-09-24T22:09:34.395946Z","shell.execute_reply.started":"2025-09-24T22:09:11.622890Z","shell.execute_reply":"2025-09-24T22:09:34.395128Z"}},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"**Step 7 : Segmentation**","metadata":{}},{"cell_type":"markdown","source":"Finally, we classify customers into business segments:\n- **Premium**: High balance customers  \n- **Active**: Medium balance & ≥ 10 transactions  \n- **Occasional**: Medium balance & < 10 transactions  \n- **LowValue**: Low balance customers  \n\nThis segmentation enables targeted marketing and personalized offers.  ","metadata":{}},{"cell_type":"code","source":"bv_segment = bv_customer_profile.join(bv_tx_by_customer, on=\"CustomerID\", how=\"left\")\n\nbv_segment = bv_segment.withColumn(\n    \"Segment\",\n    when(col(\"BalanceCategory\") == \"High\", lit(\"Premium\"))\n    .when((col(\"BalanceCategory\") == \"Medium\") & (col(\"NbTransactions\") >= 10), lit(\"Active\"))\n    .when((col(\"BalanceCategory\") == \"Medium\") & (col(\"NbTransactions\") < 10), lit(\"Occasional\"))\n    .when(col(\"BalanceCategory\") == \"Low\", lit(\"LowValue\"))\n    .otherwise(lit(\"Unknown\"))\n)\n\nBV_SEGMENT_PATH = f\"{BV_BASE}/bv_customer_segment\"\nbv_segment.write.mode(\"overwrite\").parquet(BV_SEGMENT_PATH)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T22:10:53.270939Z","iopub.execute_input":"2025-09-24T22:10:53.271630Z","iopub.status.idle":"2025-09-24T22:11:44.848021Z","shell.execute_reply.started":"2025-09-24T22:10:53.271608Z","shell.execute_reply":"2025-09-24T22:11:44.847136Z"}},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":35},{"cell_type":"markdown","source":"**Step 8 : Verification**","metadata":{}},{"cell_type":"markdown","source":"We load and preview the generated Business Vault tables to ensure the enrichment and aggregation logic was applied correctly.  \n","metadata":{}},{"cell_type":"code","source":"print(\"\\n--- Business Vault Tables Check ---\")\nfor path in [BV_CUSTOMER_PATH, BV_TX_PATH, BV_TX_MONTHLY_PATH, BV_TX_BY_CUST_PATH, BV_SEGMENT_PATH]:\n    df_tmp = spark.read.parquet(path)\n    print(f\"\\n{path}\")\n    df_tmp.show(5, truncate=False)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T22:13:15.705589Z","iopub.execute_input":"2025-09-24T22:13:15.705876Z","iopub.status.idle":"2025-09-24T22:13:16.640436Z","shell.execute_reply.started":"2025-09-24T22:13:15.705854Z","shell.execute_reply":"2025-09-24T22:13:16.639402Z"}},"outputs":[{"name":"stdout","text":"\n--- Business Vault Tables Check ---\n\n/kaggle/working/datalake/business_vault/bv_customer_profile\n+----------+----------------------------------------------------------------+-----------+----------+------------+------------------+----------------------------------------------------------------+--------------------------+------------------------+-----------+---------------+--------+\n|CustomerID|hk_customer_id                                                  |CustomerDOB|CustGender|CustLocation|CustAccountBalance|hash_diff                                                       |load_date                 |record_source           |CustomerAge|BalanceCategory|IsSenior|\n+----------+----------------------------------------------------------------+-----------+----------+------------+------------------+----------------------------------------------------------------+--------------------------+------------------------+-----------+---------------+--------+\n|C1010031  |7a82878dee0e045083d355399d710528cdaecf8a72799568b98d6dae968dc188|1984-07-21 |M         |VAPI        |1754.1            |3c0f7d5fe524a2b151b32447f232097595c736ec2f9825e4a6a1bff20ccb77cd|2025-09-24 22:00:16.368164|bank_transactions_csv_v1|41         |Medium         |false   |\n|C1010038  |98cecfe0593d2cdafd4a16eae823a9bab79058436c95af638716c72f09f42a29|1992-07-13 |F         |LOHIT       |1290.76           |2e3e95ca169eeee7cbad344e78c7c98b012246397219e6f1f413413cce719d4d|2025-09-24 22:00:16.368164|bank_transactions_csv_v1|33         |Medium         |false   |\n|C1010045  |b3f5be13ba8a7fb38ebc6e4ab691fc8b5e6b470f279e43adfb58466ce81c4660|1987-05-21 |M         |AHMEDABAD   |7051.72           |3fb5415d21cfa027ec3758469ea23340ed098f25470e297ae38f5d90d46ba29d|2025-09-24 22:00:16.368164|bank_transactions_csv_v1|38         |Medium         |false   |\n|C1010046  |4204ac52e9466cab321a3220e8f1aed04d3c51a5bc919789a7899bb023902a7e|1992-05-09 |F         |GURGAON     |2360.67           |bc43b8006f6d79b36720d9b53468411aaa106e78ae08ec30f03d1ef031484088|2025-09-24 22:00:16.368164|bank_transactions_csv_v1|33         |Medium         |false   |\n|C1010050  |a471a86ca741d5e77eb28724a17654007db710e112d328c257b3f62b1c6ba04f|1988-07-12 |F         |DELHI       |122.08            |cbbcee65e6406ec0718496fd30e8baa66d0a05c05a46900916f8acd47978d375|2025-09-24 22:00:16.368164|bank_transactions_csv_v1|37         |Low            |false   |\n+----------+----------------------------------------------------------------+-----------+----------+------------+------------------+----------------------------------------------------------------+--------------------------+------------------------+-----------+---------------+--------+\nonly showing top 5 rows\n\n\n/kaggle/working/datalake/business_vault/bv_transaction_enriched\n+-------------+---------------+---------------+-----------------+----------------------------------------------------------------+--------------------------+------------------------+----------+-------------------+------------+\n|TransactionID|TransactionDate|TransactionTime|TransactionAmount|hash_diff                                                       |load_date                 |record_source           |CustomerID|TransactionCategory|IsSuspicious|\n+-------------+---------------+---------------+-----------------+----------------------------------------------------------------+--------------------------+------------------------+----------+-------------------+------------+\n|T100         |2016-08-06     |182603         |810.0            |927fb022a0a4a5c089361154f011b8e76cb88d06a1e58c05dc2fef0a13756701|2025-09-24 22:03:02.261489|bank_transactions_csv_v1|C1713934  |Small              |false       |\n|T1000002     |2016-09-14     |114630         |30.0             |1d1942200be697a050f454a2e1a41896dba4170842d56ae296f1364a80cbd706|2025-09-24 22:03:02.261489|bank_transactions_csv_v1|C2529337  |Small              |false       |\n|T1000004     |2016-09-14     |141538         |22.25            |25a81543c6b54ce6e8c58026cbf45bbd8c8402bd7ebeff2536eaecd8e9f04051|2025-09-24 22:03:02.261489|bank_transactions_csv_v1|C5815145  |Small              |false       |\n|T1000007     |2016-09-14     |112352         |2000.0           |8f8fe103ca33c73870b74b3e643a5f6945b9c55900a9b6d8ce6621ffdba9b8e2|2025-09-24 22:03:02.261489|bank_transactions_csv_v1|C7530926  |Medium             |false       |\n|T1000009     |2016-09-14     |112032         |175.0            |95caae4f55b8309c562b6186cc9aed22b843d5d1d89918d216d40ac58dd8dea7|2025-09-24 22:03:02.261489|bank_transactions_csv_v1|C4310736  |Small              |false       |\n+-------------+---------------+---------------+-----------------+----------------------------------------------------------------+--------------------------+------------------------+----------+-------------------+------------+\nonly showing top 5 rows\n\n\n/kaggle/working/datalake/business_vault/bv_transaction_monthly\n+-------+--------+-----------------+-------------------+------------------+---------------+\n|tx_year|tx_month|TotalTransactions|TotalAmount        |AvgAmount         |ActiveCustomers|\n+-------+--------+-----------------+-------------------+------------------+---------------+\n|2016   |8       |615421           |8.957285118900098E8|1455.472776993326 |555509         |\n|2016   |9       |368994           |5.336345290200063E8|1446.187550529294 |346840         |\n|2016   |10      |3416             |6661156.989999999  |1949.9874092505852|3412           |\n+-------+--------+-----------------+-------------------+------------------+---------------+\n\n\n/kaggle/working/datalake/business_vault/bv_transaction_by_customer\n+----------+--------------+----------------------+--------------------+\n|CustomerID|NbTransactions|TotalTransactionAmount|AvgTransactionAmount|\n+----------+--------------+----------------------+--------------------+\n|C8290055  |1             |590.0                 |590.0               |\n|C8033354  |2             |587.0                 |293.5               |\n|C5021668  |1             |200.0                 |200.0               |\n|C1822946  |1             |3885.72               |3885.72             |\n|C7432485  |2             |200.0                 |100.0               |\n+----------+--------------+----------------------+--------------------+\nonly showing top 5 rows\n\n\n/kaggle/working/datalake/business_vault/bv_customer_segment\n+----------+----------------------------------------------------------------+-----------+----------+------------+------------------+----------------------------------------------------------------+--------------------------+------------------------+-----------+---------------+--------+--------------+----------------------+--------------------+----------+\n|CustomerID|hk_customer_id                                                  |CustomerDOB|CustGender|CustLocation|CustAccountBalance|hash_diff                                                       |load_date                 |record_source           |CustomerAge|BalanceCategory|IsSenior|NbTransactions|TotalTransactionAmount|AvgTransactionAmount|Segment   |\n+----------+----------------------------------------------------------------+-----------+----------+------------+------------------+----------------------------------------------------------------+--------------------------+------------------------+-----------+---------------+--------+--------------+----------------------+--------------------+----------+\n|C1010031  |7a82878dee0e045083d355399d710528cdaecf8a72799568b98d6dae968dc188|1984-07-21 |M         |VAPI        |1754.1            |3c0f7d5fe524a2b151b32447f232097595c736ec2f9825e4a6a1bff20ccb77cd|2025-09-24 22:10:53.369431|bank_transactions_csv_v1|41         |Medium         |false   |2             |1864.0                |932.0               |Occasional|\n|C1010038  |98cecfe0593d2cdafd4a16eae823a9bab79058436c95af638716c72f09f42a29|1992-07-13 |F         |LOHIT       |1290.76           |2e3e95ca169eeee7cbad344e78c7c98b012246397219e6f1f413413cce719d4d|2025-09-24 22:10:53.369431|bank_transactions_csv_v1|33         |Medium         |false   |1             |100.0                 |100.0               |Occasional|\n|C1010045  |b3f5be13ba8a7fb38ebc6e4ab691fc8b5e6b470f279e43adfb58466ce81c4660|1987-05-21 |M         |AHMEDABAD   |7051.72           |3fb5415d21cfa027ec3758469ea23340ed098f25470e297ae38f5d90d46ba29d|2025-09-24 22:10:53.369431|bank_transactions_csv_v1|38         |Medium         |false   |1             |201.0                 |201.0               |Occasional|\n|C1010046  |4204ac52e9466cab321a3220e8f1aed04d3c51a5bc919789a7899bb023902a7e|1992-05-09 |F         |GURGAON     |2360.67           |bc43b8006f6d79b36720d9b53468411aaa106e78ae08ec30f03d1ef031484088|2025-09-24 22:10:53.369431|bank_transactions_csv_v1|33         |Medium         |false   |2             |157519.74             |78759.87            |Occasional|\n|C1010050  |a471a86ca741d5e77eb28724a17654007db710e112d328c257b3f62b1c6ba04f|1988-07-12 |F         |DELHI       |122.08            |cbbcee65e6406ec0718496fd30e8baa66d0a05c05a46900916f8acd47978d375|2025-09-24 22:10:53.369431|bank_transactions_csv_v1|37         |Low            |false   |1             |248.0                 |248.0               |LowValue  |\n+----------+----------------------------------------------------------------+-----------+----------+------------+------------------+----------------------------------------------------------------+--------------------------+------------------------+-----------+---------------+--------+--------------+----------------------+--------------------+----------+\nonly showing top 5 rows\n\n","output_type":"stream"}],"execution_count":37},{"cell_type":"markdown","source":"# 6. Presentation Layer Construction","metadata":{}},{"cell_type":"markdown","source":"The **Presentation Layer** organizes data into **facts and dimensions** \nfollowing a **star schema** approach.  \nThis schema is designed for **analytics and reporting tools**.\n\nWe will create:\n1. **Dimension tables**\n   - `dim_customer` → enriched customer info with surrogate key  \n   - `dim_temps` → date dimension for time analysis  \n   - `dim_location` → geographical location dimension  \n2. **Fact table**\n   - `fact_transaction` → central fact table linking all dimensions  \n\nFinally, we **export data to Parquet and CSV** for BI tools.  \n","metadata":{}},{"cell_type":"markdown","source":"**Step 1 : Imports**","metadata":{}},{"cell_type":"code","source":"from pyspark.sql.functions import (\n    col, year, quarter, month, dayofmonth, dayofweek, date_format,\n    current_date, floor, months_between, monotonically_increasing_id\n)\nfrom pyspark.sql import functions as F\nimport os, shutil, glob\n\n# Base path for Presentation Layer\nOUTPUT_BASE = \"/kaggle/working/datalake/presentation_layer\"\nos.makedirs(OUTPUT_BASE, exist_ok=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T22:16:39.429666Z","iopub.execute_input":"2025-09-24T22:16:39.429968Z","iopub.status.idle":"2025-09-24T22:16:39.435571Z","shell.execute_reply.started":"2025-09-24T22:16:39.429947Z","shell.execute_reply":"2025-09-24T22:16:39.434673Z"}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"**Step 2 : DIMENSIONS**","metadata":{}},{"cell_type":"markdown","source":"- **dim_customer**: contains customer information enriched with segmentation, \n  plus a **surrogate key (`customer_sk`)**.  \n- **dim_temps**: date dimension for time-based analysis (year, quarter, month, weekday).  \n- **dim_location**: unique list of customer locations with surrogate key.  \n","metadata":{}},{"cell_type":"code","source":"# --- Dim Customer ---\ndim_customer = bv_segment.select(\n    col(\"CustomerID\").alias(\"customer_id_bk\"),  # Business Key\n    col(\"CustomerDOB\").alias(\"date_of_birth\"),\n    col(\"CustGender\").alias(\"gender\"),\n    col(\"CustLocation\").alias(\"location\"),\n    col(\"CustAccountBalance\").alias(\"account_balance\"),\n    col(\"CustomerAge\").alias(\"age\"),\n    col(\"BalanceCategory\").alias(\"balance_category\"),\n    col(\"IsSenior\").alias(\"is_senior\"),\n    col(\"Segment\").alias(\"segment\")\n).dropDuplicates([\"customer_id_bk\"]) \\\n .withColumn(\"customer_sk\", monotonically_increasing_id())  # Surrogate Key\n\ndim_customer.write.mode(\"overwrite\").parquet(f\"{OUTPUT_BASE}/dim_customer\")\n\n\n# --- Dim Temps ---\ndim_temps = tx_with_customer.select(col(\"TransactionDate\").alias(\"date\")) \\\n    .dropDuplicates([\"date\"]) \\\n    .withColumn(\"year\", year(col(\"date\"))) \\\n    .withColumn(\"quarter\", quarter(col(\"date\"))) \\\n    .withColumn(\"month\", month(col(\"date\"))) \\\n    .withColumn(\"day\", dayofmonth(col(\"date\"))) \\\n    .withColumn(\"day_of_week\", dayofweek(col(\"date\"))) \\\n    .withColumn(\"weekday_name\", date_format(col(\"date\"), \"EEEE\")) \\\n    .withColumn(\"date_sk\", monotonically_increasing_id())\n\ndim_temps.write.mode(\"overwrite\").parquet(f\"{OUTPUT_BASE}/dim_temps\")\n\n\n# --- Dim Location ---\ndim_location = bv_customer_profile.select(\n    col(\"CustLocation\").alias(\"location\")\n).dropDuplicates([\"location\"]) \\\n .withColumn(\"location_sk\", monotonically_increasing_id())\n\ndim_location.write.mode(\"overwrite\").parquet(f\"{OUTPUT_BASE}/dim_location\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T22:17:31.362905Z","iopub.execute_input":"2025-09-24T22:17:31.363592Z","iopub.status.idle":"2025-09-24T22:18:48.384871Z","shell.execute_reply.started":"2025-09-24T22:17:31.363569Z","shell.execute_reply":"2025-09-24T22:18:48.383997Z"}},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":39},{"cell_type":"markdown","source":"**Step 3 : FACT TABLE**","metadata":{}},{"cell_type":"markdown","source":"The **fact_transaction** table is the central fact table, linked to all dimensions:  \n- `transaction_id` (business key)  \n- `customer_sk`, `date_sk`, `location_sk` (foreign keys to dimensions)  \n- `amount`, `transaction_category`, `is_suspicious` (measures and attributes)  \n","metadata":{}},{"cell_type":"code","source":"# ========================================\n# 3. FACT TABLE\n# ========================================\n\nfact_transaction = tx_with_customer \\\n    .join(dim_customer, tx_with_customer.CustomerID == dim_customer.customer_id_bk, \"left\") \\\n    .join(dim_temps, tx_with_customer.TransactionDate == dim_temps.date, \"left\") \\\n    .join(dim_location, dim_customer.location == dim_location.location, \"left\") \\\n    .select(\n        col(\"TransactionID\").alias(\"transaction_id\"),\n        col(\"customer_sk\"),   # FK to dim_customer\n        col(\"date_sk\"),       # FK to dim_temps\n        col(\"location_sk\"),   # FK to dim_location\n        col(\"TransactionTime\").alias(\"time\"),\n        col(\"TransactionAmount\").alias(\"amount\"),\n        col(\"TransactionCategory\").alias(\"transaction_category\"),\n        col(\"IsSuspicious\").alias(\"is_suspicious\")\n    ).dropDuplicates([\"transaction_id\"])\n\nfact_transaction.write.mode(\"overwrite\").parquet(f\"{OUTPUT_BASE}/fact_transaction\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T22:20:32.215846Z","iopub.execute_input":"2025-09-24T22:20:32.216678Z","iopub.status.idle":"2025-09-24T22:21:35.706610Z","shell.execute_reply.started":"2025-09-24T22:20:32.216649Z","shell.execute_reply":"2025-09-24T22:21:35.704958Z"}},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":40},{"cell_type":"markdown","source":"**Step 4 : EXPORT TO CSV**","metadata":{}},{"cell_type":"markdown","source":"Each table (dimensions and fact) is exported to **Parquet** (optimized storage) \nand **CSV** (for BI tools).  \n\nThe CSV files are stored in:  \n`/kaggle/working/datalake/presentation_layer_csv/`  \n","metadata":{}},{"cell_type":"code","source":"EXPORT_PATH = \"/kaggle/working/datalake/presentation_layer_csv\"\nos.makedirs(EXPORT_PATH, exist_ok=True)\n\ndef export_to_csv_named(df, filename, export_dir=EXPORT_PATH):\n    temp_dir = os.path.join(export_dir, f\"temp_{filename}\")\n    df.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(temp_dir)\n    csv_file = glob.glob(os.path.join(temp_dir, '*.csv'))[0]\n    final_path = os.path.join(export_dir, f\"{filename}.csv\")\n    shutil.move(csv_file, final_path)\n    shutil.rmtree(temp_dir)\n    print(f\"✅ Export {filename} terminé : {final_path}\")\n\n# Export dimensions and fact\nexport_to_csv_named(dim_customer, \"dim_customer\")\nexport_to_csv_named(dim_temps, \"dim_temps\")\nexport_to_csv_named(dim_location, \"dim_location\")\nexport_to_csv_named(fact_transaction, \"fact_transaction\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-24T22:22:29.239194Z","iopub.execute_input":"2025-09-24T22:22:29.239493Z","iopub.status.idle":"2025-09-24T22:24:57.849409Z","shell.execute_reply.started":"2025-09-24T22:22:29.239472Z","shell.execute_reply":"2025-09-24T22:24:57.848107Z"}},"outputs":[{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"✅ Export dim_customer terminé : /kaggle/working/datalake/presentation_layer_csv/dim_customer.csv\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"✅ Export dim_temps terminé : /kaggle/working/datalake/presentation_layer_csv/dim_temps.csv\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"},{"name":"stdout","text":"✅ Export dim_location terminé : /kaggle/working/datalake/presentation_layer_csv/dim_location.csv\n","output_type":"stream"},{"name":"stderr","text":"[Stage 296:>                                                        (0 + 1) / 1]\r","output_type":"stream"},{"name":"stdout","text":"✅ Export fact_transaction terminé : /kaggle/working/datalake/presentation_layer_csv/fact_transaction.csv\n","output_type":"stream"},{"name":"stderr","text":"                                                                                \r","output_type":"stream"}],"execution_count":41},{"cell_type":"markdown","source":"# 7. Power BI Dashboard","metadata":{}},{"cell_type":"markdown","source":"Once the **CSV exports** were generated from the Presentation Layer,  \nI imported them into **Power BI** to build a dashboard.  \n\n### Process:\n1. Import the CSV files:\n   - `dim_customer.csv`\n   - `dim_temps.csv`\n   - `dim_location.csv`\n   - `fact_transaction.csv`\n\n2. Define relationships between tables (Star Schema):\n   - `fact_transaction.customer_sk` → `dim_customer.customer_sk`\n   - `fact_transaction.date_sk` → `dim_temps.date_sk`\n   - `fact_transaction.location_sk` → `dim_location.location_sk`\n\n3. Build visuals:\n   - Transaction trends over time\n   - Customer segmentation by balance and transactions\n   - Suspicious transaction detection\n   - Geographic distribution of customers\n\nThis dashboard provides **business users** with easy-to-understand insights.\n","metadata":{}},{"cell_type":"markdown","source":"# 8. Conclusion","metadata":{}},{"cell_type":"markdown","source":"This project implemented a **complete Data Vault pipeline**:\n\n1. **Raw Vault Layer**: captured data in a traceable, historical format.  \n2. **Business Vault Layer**: enriched data with business rules and aggregates.  \n3. **Presentation Layer**: structured data into facts and dimensions for BI.  \n4. **Power BI Dashboard**: delivered actionable insights to end users.  \n\n### Key Takeaways:\n- **Data Vault methodology** ensures *scalability, auditability, and adaptability*.  \n- The separation between **raw data** and **business logic** makes the model flexible.  \n- With the **Presentation Layer**, business users can access clean data directly in BI tools.  \n\n### Importance:\nThis approach demonstrates how **modern data warehousing** can transform raw transactions  \ninto **high-value analytics**, enabling better **decision-making** in banking and finance.  \n","metadata":{}}]}